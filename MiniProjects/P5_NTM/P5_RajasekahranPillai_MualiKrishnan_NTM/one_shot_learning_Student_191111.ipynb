{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"one_shot_learning_Student_191111.ipynb","provenance":[{"file_id":"17BvkhrvTKBi0IMM1Fazl6UVLtm0fb8E7","timestamp":1573460220489},{"file_id":"1tRHaZoIYsfB16Gh-s_KAX6A41NdP3XKa","timestamp":1573332369516},{"file_id":"1EfocEfdlVns48iXbXJf7_Jy-v-WCN9NN","timestamp":1573327062889}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"Ep1Ky-nDBZIr","colab_type":"text"},"source":["You might need to modify the third line in the code cell below, to make sure you cd to the actual directory which your ipynb file is located in.\n","\n","**Caution**: due to the nature of this project's setup, everytime you want to rerun some code cell below, please click **Runtime -> Restart and run all**; this operation clears the computational graphs and the local variables but allow training and testing data that are already loaded from google drive to stay in the colab runtime space. Please do **not** do the following if you just wish to rerun code: click Runtime -> reset all runtimes, and then click Runtime -> Run all; it will remount your google drive, and remove the training and testing data already loaded in your colab runtime space. **Runtime -> Restart and run all** automatically avoids remounting the drive after the first time you run the notebook file; the loaded data can usually stay in your colab runtime space for many hours.\n","\n","Loading the training and testing data after remounting your google drive takes 30 - 40 minutes."]},{"cell_type":"code","metadata":{"id":"7sWni0FseVUz","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount(\"/content/gdrive/\", force_remount=True)\n","%cd gdrive/My Drive/Neural_Turing_Machine/NTM_small"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"R9gdekJg_-xa","colab_type":"code","colab":{}},"source":["from utils import OmniglotDataLoader, one_hot_decode, five_hot_decode\n","import tensorflow as tf\n","import argparse\n","import numpy as np\n","%tensorflow_version 1.x\n","print(tf.__version__)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"efHLdzvkAKfK","colab_type":"text"},"source":["The following class `MANNCell` is the core of the memory-augmented neural network (MANN). You will implement the main parts of it in Tensorflow 2.0.\n","\n","Before any technical discussion of how the MANNCell should operate, let us look at what it should do on a general level. Suppose we have an input batch of 16 episodes of image samples, with each episode being of equal length of 50. Based on the design of the rest of the project (which we have already implemented for you), MANNCell should be called 50 times, each time having 16 input samples (along with the offseted labels), and outputting 16 output labels. More specifically, the MANNCell should produce classification labels $[\\hat{y}_0^t, ..., \\hat{y}_{15}^t]$ for all 16 iteration-$t$ image samples batch $[x^t_0+\\text{null}, x^t_1+y_0^t, ..., x^t_{15}+y_{14}^t]$ (\"+\" means concatenation) every time it is called; for your information, it is the class NTMOneShotLearningModel (already implemented below) that actually calls MANNCell 50 times. Your job is to make sure that at a single iteration $t$ (where $t=0,1,2,...,49$), MANNCell correctly parses the input arguments, produce the correct read and write weights $w^r_t, w^w_t$, correctly retrieve from and write to the memory to form $M_t$, and use the right material to get the logits for classification (they will be used for computing the labels and cross-entropy values in NTMOneShotLearningModel), and return the right states that will be used in the next iteration $t+1$. \n","\n","Let us look at the input arguments of the method `call(self, inputs, states)`  of this class first:\n","*   The `inputs` variable shall have the following shape: \n","    `(batch_size, image_size+num_classes)`. \n","  *   It corresponds to the $[x^t_0+\\text{null}, x^t_1+y^t_0, ..., x^t_{15}+y^t_{14}]$ above, for some iteration $t=0,1,...,49$.\n","  *   `inputs[p,:]` is the $p$-th image in the batch `inputs` (note that the images are flattened to 1D tensors, and the labels are one-hot encoded).\n","*   The `states` variable is a dictionary that has the following set of keys:`{'controller_state', 'read_vector_list', 'w_r_list', 'w_u', 'M'}`\n","  *   `controller_state` is the state of the controller in iteration $t-1$; if $t-1 < 0$, then it is just zero-filled. As it is an LSTM cell, `controller_state` is of the form `[(batch_size, rnn_size),(batch_size, rnn_size)]` (technically speaking its shape is `(2, batch_size, rnn_size)`). The two `(batch_size, rnn_size)`-shaped entries in it correspond to the cell state and the hidden state of the LSTM. We will mostly be treating the LSTM controller as a black-box in this project, so we do not need to pay much attention to the details of its states. If interested, you can read about the LSTM cell's technical details in [tf.keras.layers.LSTMCell](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTMCell).\n","  *   `read_vector_list` is the list of read vectors $r_{t-1}$ which we obtained in the previous iteration $t-1$ in the episode; if $t-1 < 0$, then the read vector list is initialized to be an arbitrary one-hot vector. It is of the shape `(head_num, batch_size, memory_vector_dim)`. Basically, `read_vector_list[i,p,:]` is the $(t-1)$-th-iteration read vector of the $i$-th read head for the $p$-th input sample in the batch.\n","  *   `w_r_list` is the list of read weights $w^r_{t-1}$ which we obtained in the previous iteration $t-1$ in the episode; if $t-1 < 0$, then the read weights list is initialized to be an arbitrary one-hot vector. It is of the shape `(head_num, batch_size, num_memory_slots)`. Basically, `w_r_list[i,p,:]` is the $(t-1)$-th-iteration read weight of the $i$-th read head for the $p$-th input sample in the batch.\n","  *   `w_u` is the list of memory usage weights $w^u_{t-1}$ which we obtained in the previous iteration $t-1$ in the episode; if $t-1 < 0$, then the usage weights list is is initialized to be an arbitrary one-hot vector. It is of the shape `(batch_size, num_memory_slots)`. Basically, `w_u[p,:]` is the $(t-1)$-th-iteration memory usage weight of the $p$-th input sample in the batch.\n","  *   `M` is the memory content from the previous iteration $t-1$; if $t-1 < 0$, then the memory is just zero-filled. It is of shape `(batch_size, num_memory_slots, memory_vector_dim)`. Basically, `M[p,j,:]` is the $j$-th memory vector in the memory block for the $p$-th sample in the batch from iteration $t-1$, and `M[p,:,:]` is the memory block for the $p$-th sample in the batch, where the memory block is a 2D structure that has `num_memory_slots` memory vectors, each vector of length `memory_vector_dim`.\n","\n","\n","\n","\n","    \n"]},{"cell_type":"markdown","metadata":{"id":"ivOh5NlfRtC3","colab_type":"text"},"source":["\n","\n","---\n","\n","\n","\n","Now let us look at some of the technical details of the MANNCell. First, we discuss the main ingredients of the MANNCell, and initialization of the relevant units.\n","*   The input arguments of the class initialization method `__init__` have already been specified, they will be used to initialize relevant structures in the class.\n","*   `self.controller`: this is the controller of the MANN cell that is responsible for interfacing with the memory $M$. We recommend using `tf.keras.layers.LSTMCell` with `units=rnn_size` for initialization. For its technical details, see [tf.keras.layers.LSTMCell\n","](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTMCell).\n","*   `self.controller_output_to_read_keys`, `self.controller_output_to_write_keys`, `self.controller_output_to_alphas`: the LSTM controller's output structure (we will discuss what its inputs should be later) is of the form [controller_output, controller_cell_and_hidden_states]. We need a mapping that maps the controller_output to the read keys, write keys and the interpolation coefficient $\\alpha_t$'s, which will then be used for interacting with the memory. Three `tf.keras.layers.Dense` layers (one for producing read keys, one for write keys, one for the $\\alpha_t$'s) are sufficient, though you are welcome to try out more complicated structures. \n","  *  **Remark 1**: each access to memory involves `head_num` number of heads, if you wish, you could just initialize `self.controller_output_to_read_keys` with `units=self.memory_vector_dim*self.head_num` and apply `tf.split` to the output of the dense layer along `axis=1` and `num_or_size_splits=head_num` in the `call` method (similar for the other two dense layers).\n","  *  ***From now on, we assume that you are following Remark 1 above in your implementation***.\n","*    `self.controller_output_to_logits`: it should be a dense layer that will be used to map the concatenated controller_output + read_vector_list to the logits that will be used for obtaining the classification labels of the inputs and computing the cross entropy values. Thus, initialize it with `units=self.num_classes`."]},{"cell_type":"markdown","metadata":{"id":"bMM2bJpSR3EP","colab_type":"text"},"source":["---\n","\n","Finally, we discuss how to implement the method `call`. The following discussion is only one way of implementing the method, please feel free to deviate from it. However, ***we do suggest you to at least read through the discussion once***, as we have already implemented parts of the method and the whole training loop for you, and incompatibility between the data structures could cause the code to not run or have buggy outputs.\n","*   **Caution**: even though most of the discussion below that involve tensors are treated either element-wise or vector-wise, in your implementation please utitlize tensorflow matrix operations as much as possible, as it can avoid strange bugs and increase the speed of your model.\n","*   As described before, the input arguments of the `call` method are `inputs` and `states`. \n","  *  Parse `state` to obtain `prev_controller_state`, `prev_read_vector_list`, `prev_w_r_list`, `prev_w_u`, `prev_M` that come from the previous iteration $t-1$. You may assume that they are zero-filled if $t=0$.\n","*  Constructing the controller's input had been implemented for you. \n","  *  The controller's output will be of the form `(controller_output, controller_states)`.\n","  *  Why do you think  we should involve `prev_read_vector_list` in the controller's input?\n","*  Now pass `controller_output` to the dense layers we discussed before, and obtain the read keys, write keys and the interpolation coefficients.\n","  * Following the suggestion in the Remark 1 above, after applying `tf.split` to the dense layers' outputs, the shapes of your `read_key_list` and `write_key_list` should both be `(head_num, batch_size, memory_vector_dim)`, and the shape of `alpha_list` should be `(head_num, batch_size, 1)`. As an example, `read_key_list[i,p,:]` should be the memory read key for the $i$-th read head for the $p$-th sample in the batch.\n","\n","*  Before computing the read and write weights and interact with memory, we need to compute `prev_w_lu`, the least used weights from the previous iteration $t-1$. \n","  *  You need to fill in the code for method `compute_w_lu`. To compute `prev_w_lu`, note that for the $p$-th sample in the batch in the previous iteration $t-1$, `prev_w_lu[p,:]` is a vector of binary values with length `num_memory_slots`: defining \n","     \\begin{equation}\n","      s(\\text{prev_w_u}[p,:], k)= \\text{the $k$-th smallest entry in prev_w_u}[p,:]\n","     \\end{equation}\n","     we have \n","     \\begin{equation}\n","      \\text{prev_w_lu}[p,i] = 0, \\;\\; \\text{if prev_w_u}[p,i] > s(\\text{prev_w_u}[p,:], \\text{head_num})\n","     \\end{equation}\n","     and \n","     \\begin{equation}\n","      \\text{prev_w_lu}[p,i]=1 \\;\\; \\text{otherwise}\n","     \\end{equation}\n","  *   Here is one way to implement `compute_w_lu`. Given input argument `prev_w_u` the usage weight from the previous iteration $t-1$ (it has shape `(batch_size, num_memory_slots)`), use `tf.math.top_k` to obtain the desired set of indices from `prev_w_u` (so you should have a `batch_size` number of index sets, each set is of size `head_num`; the overall structure should be of shape `(batch_size, head_num)`). Then use `tf.one_hot` and `tf.reduce_sum` to expand these indices into `prev_w_lu`, which should have shape `(batch_size, num_memory_slots)`. \n","    *  From the set of indices with size `(batch_size, head_num)` you used for computing `prev_w_lu`, remember to also construct and return the index corresponding to *the smallest* entry in `prev_w_u[p,:]` for every $p$ (this index also correspond to the memory slot that was least used for the $p$-th sample in the previous iteration); so your returned indices will have size `(batch_size, 1)`.\n","    *  You may find [tf.math.top_k\n","](https://www.tensorflow.org/api_docs/python/tf/math/top_k), [tf.one_hot\n","](https://www.tensorflow.org/api_docs/python/tf/one_hot) and [tf.reduce_sum\n","](https://www.tensorflow.org/api_docs/python/tf/math/reduce_sum)  useful.\n","    \n","\n","*  Now we proceed to compute the read and write weights $w^r_t$ and $w^w_t$.\n","  *  For the $p$-th sample in the batch, recall that the read key `read_key_list[m,p,:]` is for the $m$-th read head for that sample, and `prev_M[p,j,:]` is the $j$-th memory vector for the $p$-th sample from the previous interation $t-1$ . Then the memory **read** weight `w_r_list[m,p,:]` for the $m$-th read head for the $p$-th sample is a 1D tensor with length `num_memory_slots`, with entries\n","  \\begin{equation}\n","    \\text{w_r_list}[m,p,i] = \\frac{\\exp(K(\\text{prev_M}[p,i,:],\\text{read_key_list}[m,p,:]))}{\\sum_{j=0}^{\\text{num_memory_slots}-1}\\exp(K(\\text{prev_M}[p,j,:], \\text{read_key_list}[m,p,:]))}\n","  \\end{equation}\n","  where $i=0,1,...,\\text{num_memory_slots}-1\\$, and\n","  \\begin{equation}\n","    K(x, y) = \\frac{x\\cdot y}{\\Vert x \\Vert_2 \\Vert y \\Vert_2 + \\epsilon}\n","  \\end{equation}\n","    *  $\\epsilon$ is there to ensure numerical stability. $\\epsilon=10^{-8}$ seems to be a good choice.\n","    *  You might find some of the following tensorflow operations useful: [tf.matmul\n","](https://www.tensorflow.org/api_docs/python/tf/linalg/matmul), [tf.norm\n","](https://www.tensorflow.org/api_docs/python/tf/norm), [tf.expand_dims\n","](https://www.tensorflow.org/api_docs/python/tf/expand_dims), [tf.squeeze\n","](https://www.tensorflow.org/api_docs/python/tf/squeeze), [tf.math.exp\n","](https://www.tensorflow.org/api_docs/python/tf/math/exp) \n","\n","    *  In the suggested setup, the method `compute_read_weights`'s return shape should be `(batch_size, num_memory_slots)`, and `w_r_list` should have shape `(head_num, batch_size, num_memory_slots)`.\n","\n","  *  Given the $p$-th sample in the batch, the memory **write** weight `w_w_list[m,p,:]` for the $m$-th write head for that sample is of the general form:\n","     \\begin{equation}\n","      \\text{w_w_list}[m,p,i] = \\text{Sigmoid}(\\text{alpha_list}[m,p,0])\\times\\text{prev_w_r_list}[m,p,i] + (1 - \\text{Sigmoid}(\\text{alpha_list}[m,p,0]))\\times\\text{prev_w_lu}[p,i]\n","     \\end{equation}\n","     where $i=0,...,\\text{num_memory_slots-1}$.\n","    *  In our suggested setup, method `compute_write_weights`'s return shape should be `(batch_size, num_memory_slots)`, so `w_w_list` should have shape `(head_num, batch_size, num_memory_slots)`.\n","\n","*  Let us read from memory `prev_M` now.\n","    *  As we have `w_r_list` with shape `(head_num, batch_size, num_memory_slots)`, to obtain the read vectors, simply carry out the following: for the $m$-th read head for the $p$-th sample, \n","      \\begin{equation}\n","        \\text{read_vector_list}[m,p,:] = \\sum_{j=0}^{\\text{num_memory_slots}-1}\\text{w_r_list}[m,p,j]\\times\\text{prev_M}[p,j,:]\n","      \\end{equation}\n","      where `read_vector_list` has shape `(head_num, batch_size, memory_vector_dim)`.\n","      *  Please remember that computing with matrices (in contrast to using some kind of for loop) can usually make you code run faster.\n","\n","* Having obtained the write weights `w_w_list`, we are closer to accessing the content of the memory now. But before that, rememeber that we got a set of indices of size `(batch_size, 1)` from the method `compute_w_lu` that indicated the least used memory slot in the previous iteration $t-1$? We are going to use them to zero out *the least used slot* in the memory first, before the writing operations.\n","  *  One way of implementation: apply `tf.one_hot` to the set of indices of size `(batch_size, 1)` to obtain a matrix `E` of size `(batch_size, num_memory_slots)` containing one-hot vectors, where `E[p,j]` is 1 if the $j$-th memory slot for the $p$-th sample in the previous iteration was least used. Then we just need to compute the new memory along the line of $M*(1-E)$. So we have obtained `M_erased`, with shape `(batch_size, num_memory_slots, memory_vector_dim)`.\n","\n","* Now we can write to memory:\n","  *  Recall that we have already computed `write_key_list` and `w_w_list` with shapes `(head_num, batch_size, memory_vector_dim)` and `(head_num, batch_size, num_memory_slots)` respectively. To write to `M_erased` with the $m$-th write head for the $p$-th sample, simply compute\n","     \\begin{equation}\n","      \\text{M_written}[p,i,:] = \\text{M_erased}[p,i,:] + \\text{w_w_list}[m,p,i]\\times\\text{write_key_list}[m,p,:]\n","     \\end{equation}\n","      *  You might find [tf.matmul\n","](https://www.tensorflow.org/api_docs/python/tf/linalg/matmul) and [tf.expand_dims\n","](https://www.tensorflow.org/api_docs/python/tf/expand_dims) useful here.\n","  \n","\n","*  Finally, update the usage weight $w^u_t$ following the formula: for the $p$-th sample in the batch,\n","   \\begin{equation}\n","     \\text{w_u}[p,:] = \\text{self.gamma}\\times\\text{prev_w_u}[p,:] + \\sum_{i=0}^{\\text{head_num}-1}\\text{w_r_list}[i,p,:] + \\sum_{i=0}^{\\text{head_num}-1}\\text{w_w_list}[i,p,:]\n","   \\end{equation}\n","   where `w_u` has shape `(batch_size, num_memory_slots)`, and `self.gamma` is a manually defined free parameter of the model, which we have already set for you.\n","*  Finally, we update the `state` dictionary , and feed [controller's output + the read vector list] to `self.controller_output_to_logits` which will be used for obtaining the labels for the input samples (already written for you) . Please ensure that all the relevant tensors have the correct shape and content.\n"]},{"cell_type":"code","metadata":{"id":"Kwt3noNe_0PU","colab_type":"code","colab":{}},"source":["class MANNCell(tf.keras.layers.AbstractRNNCell):\n","  def __init__(self, rnn_size, num_memory_slots, memory_vector_dim, head_num, num_classes=5, gamma=0.95, **kwargs):\n","    super().__init__(**kwargs)\n","    ################ Setup ###############################################\n","    self.rnn_size = rnn_size\n","    # number of memory slots\n","    self.num_memory_slots = num_memory_slots\n","    # size of each memory slot\n","    self.memory_vector_dim = memory_vector_dim\n","    self.head_num = head_num\n","    # memory access head number is the same for both read and \n","    # write in our setup  \n","    self.write_head_num = head_num\n","    # decay parameter for computing the usage weights\n","    self.gamma = gamma\n","\n","    self.num_classes = num_classes\n","    ########################################################################\n","\n","    # Controller RNN layer, we use an LSTM\n","    # Recommended: tf.keras.layers.LSTMCell\n","    self.controller = # Fill in\n","\n","    # controller_output \n","    #          -> read_key (batch_size, head_num*memory_vector_dim)\n","    #          -> write_key (batch_size, head_num*memory_vector_dim)\n","    #          -> alpha (batch_size, head_num), interpolation coefficient for writing to memory\n","    #\n","    # We suggest units=self.memory_vector_dim*self.head_num for initializing the dense layers\n","    # for read key and write keys, and units=self.head_num for the dense layer for alpha,\n","    # and apply tf.split along axis=1 in the call method\n","    self.controller_output_to_read_keys = # Fill in\n","    self.controller_output_to_write_keys = # Fill in\n","    self.controller_output_to_alpha = # Fill in\n","\n","    # This is the dense layer for mapping the controller output + read vector list to \n","    # logits (which will then be used for computing the labels and cross-entropy values\n","    # in NTMOneShotLearningModel). So initialize it with units=self.num_classes.\n","    self.controller_output_to_logits = # Fill in\n","\n","  @property\n","  def state_size(self):\n","    return self.rnn_size\n","\n","  # This initializes the dictionary states in MANNCell, and returns the initial state.\n","  # Please do not change it.\n","  def zero_state(self, batch_size, rnn_size, dtype):\n","    one_hot_weight_vector = np.zeros([batch_size, self.num_memory_slots])\n","    one_hot_weight_vector[..., 0] = 1\n","    one_hot_weight_vector = tf.constant(one_hot_weight_vector, dtype=tf.float32)\n","    initial_state = {\n","            'controller_state': [tf.zeros((batch_size, rnn_size)), tf.zeros((batch_size, rnn_size))],\n","            'read_vector_list': [tf.zeros([batch_size, self.memory_vector_dim])\n","                                  for _ in range(self.head_num)],\n","            'w_r_list': [one_hot_weight_vector for _ in range(self.head_num)],\n","            'w_u': one_hot_weight_vector,\n","            'M': tf.constant(np.ones([batch_size, self.num_memory_slots, self.memory_vector_dim]) * 1e-6, dtype=tf.float32)\n","        }\n","    return initial_state\n","\n","  def call(self, inputs, states):\n","    # read vectors from the previous iteration, extract from states\n","    prev_read_vector_list = # Fill in \n","    # state of controller from previous iteration t-1, extract from states\n","    prev_controller_state = # Fill in  \n","    # Obtain the list of w^r_{t-1}, M_{t-1}, and w^u_{t-1}, extract from states\n","    prev_w_r_list = # Fill in\n","    prev_M = # Fill in\n","    prev_w_u = # Fill in\n","\n","    # Controller output form the parameters of the read and write vectors\n","    controller_input = tf.concat([inputs] + prev_read_vector_list, axis=1)\n","    controller_output, controller_state = self.controller(inputs=controller_input, states=prev_controller_state)\n","\n","    # Map the controller_output to the read_keys, write_keys, and alphas\n","    read_keys = # Fill in\n","    write_keys = # Fill in\n","    alphas = # Fill in\n","\n","    # We have head_num heads per access to memory (same number of heads for read and write),\n","    # so split the parameters obtained above into head_num groups, \n","    # tf.split is useful here (try splitting along axis=1. Why?)\n","    read_key_list = tf.tanh(# Fill in tf.split operation)\n","    write_key_list = tf.tanh(# Fill in tf.split operation)\n","    sig_alpha = tf.sigmoid(# Fill in tf.split operation)\n","\n","    # For every p-th sample in the batch (from iteration t-1), compute the index \n","    # corresponding to least used memory slot in prev_M[p,:,:], return as prev_indices.\n","    # Also compute w^lu_{t-1}, return as prev_w_lu.\n","    # Please fill in the method self.compute_w_lu.  \n","    prev_indices, prev_w_lu = self.compute_w_lu(prev_w_u)\n","\n","    # Setup read and write weights\n","    w_r_list = []\n","    w_w_list = []\n","    # We obtain read and write weights for each head\n","    for i in range(self.head_num):\n","      # Obtain READ weights\n","      # Please fill in the method self.compute_read_weights\n","      w_r = self.compute_read_weights(read_key_list[i], prev_M)\n","      # Obtain WRITE weights\n","      # Please fill in the method self.compute_write_weights\n","      w_w = self.compute_write_weights(sig_alpha[i], prev_w_r_list[i], prev_w_lu)\n","      # Note: w_r_list is of shape (head_num, batch_size, num_memory_slots), \n","      # and same for w_w_list\n","      w_r_list.append(w_r)\n","      w_w_list.append(w_w)\n","\n","\n","    # Read from memory M_{t-1}, using the w_r_list\n","    read_vector_list = []\n","    # Iterate over each head\n","    for i in range(self.head_num):\n","      # Fill in, compute read_vector\n","      # read_vector_list should have shape (head_num, batch_size, memory_vector_dim)\n","      read_vector_list.append(read_vector)\n","\n","\n","    # Set least used memory slot in prev_M to ZERO, make use of prev_indices!\n","    M_erased = # Fill in\n","\n","    # Write to memory, form M_t, using the w_w_list and write_keys\n","    # Iterate over each head\n","    for i in range(self.head_num):\n","      # Fill in\n","      M_written = # Fill in\n","    \n","    # Compute usage weights w^u_t for the current iteration\n","    w_u = # Fill in\n","\n","    # Concatenate controller's output and the read memory\n","    # content, they are then fed into a dense layer to obtain the logits,\n","    # which will be used for obtaininig labels and computing the  cross-entropy \n","    # values in NTMOneShotLearningModel below\n","    mann_output = tf.concat([controller_output] + read_vector_list, axis=1)\n","    logits = # Fill in\n","\n","    state = {\n","        'controller_state': controller_state,\n","        'read_vector_list': read_vector_list,\n","        'w_r_list': w_r_list,\n","        'w_w_list': w_w_list,\n","        'w_u': w_u,\n","        'M': M_written,\n","    }\n","\n","    return logits, state\n","\n","  def compute_read_weights(self, read_key, prev_M):\n","    # Fill in\n","\n","    # Compute the inner products, norms\n","\n","    # Compute the exp(K(M,key))'s\n","    \n","    # Obtain read weights\n","\n","    return w_r\n","\n","  def compute_write_weights(self, sig_alpha, prev_w_r, prev_w_lu):\n","    # Compute the write weights\n","    # Fill in\n","\n","    return w_w\n","\n","  def compute_w_lu(self, prev_w_u):\n","    # Fill in\n","    return indices, prev_w_lu\n","  "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"otNm4yidAQQB","colab_type":"text"},"source":["Already implemented, no need to change.\n","\n","This class is part of the training loop."]},{"cell_type":"code","metadata":{"id":"MZTXPodW_5_i","colab_type":"code","colab":{}},"source":["class NTMOneShotLearningModel():\n","  def __init__(self, model, n_classes, batch_size, seq_length, image_width, image_height,\n","                rnn_size, num_memory_slots, rnn_num_layers, read_head_num, write_head_num, memory_vector_dim, learning_rate):\n","    self.output_dim = n_classes\n","\n","    # Note: the images are flattened to 1D tensors\n","    # The input data structure is of the following form:\n","    # self.x_image[i,j,:] = jth image in the ith sequence (or, episode)\n","    self.x_image = tf.placeholder(dtype=tf.float32, shape=[batch_size, seq_length, image_width * image_height])\n","    # Model's output label is one-hot encoded\n","    # The data structure is of the following form:\n","    # self.x_label[i,j,:] = one-hot label of the jth image in \n","    #             the ith sequence (or, episode)\n","    self.x_label = tf.placeholder(dtype=tf.float32, shape=[batch_size, seq_length, self.output_dim])\n","    # Target label is one-hot encoded\n","    self.y = tf.placeholder(dtype=tf.float32, shape=[batch_size, seq_length, self.output_dim])\n","\n","    if model == 'LSTM':\n","      # Using a LSTM layer to serve as the controller, no memory\n","      def rnn_cell(rnn_size):\n","        return tf.nn.rnn_cell.BasicLSTMCell(rnn_size)\n","      cell = tf.nn.rnn_cell.MultiRNNCell([rnn_cell(rnn_size) for _ in range(rnn_num_layers)])\n","      state = cell.zero_state(batch_size=batch_size, dtype=tf.float32)\n","    elif model == 'MANN':\n","      # Using a MANN network as the controller, with memory\n","      cell = MANNCell(rnn_size, num_memory_slots, memory_vector_dim,\n","                                head_num=read_head_num)\n","      state = cell.zero_state(batch_size=batch_size, rnn_size=rnn_size, dtype=tf.float32)\n","    \n","    \n","    self.state_list = [state]\n","    # Setup the NTM's output\n","    self.o = []\n","    \n","    # Now iterate over every sample in the sequence \n","    for t in range(seq_length):\n","      output, state = cell(tf.concat([self.x_image[:, t, :], self.x_label[:, t, :]], axis=1), state)\n","      output = tf.nn.softmax(output, axis=1)\n","      self.o.append(output)\n","      self.state_list.append(state)\n","    # post-process the output of the classifier\n","    self.o = tf.stack(self.o, axis=1)\n","    self.state_list.append(state)\n","\n","    eps = 1e-8\n","    # cross entropy, between model output labels and target labels\n","    self.learning_loss = -tf.reduce_mean(  \n","        tf.reduce_sum(self.y * tf.log(self.o + eps), axis=[1, 2])\n","    )\n","    \n","    self.o = tf.reshape(self.o, shape=[batch_size, seq_length, -1])\n","    self.learning_loss_summary = tf.summary.scalar('learning_loss', self.learning_loss)\n","\n","    with tf.variable_scope('optimizer'):\n","      self.optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n","      self.train_op = self.optimizer.minimize(self.learning_loss)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"X_qMlbTWAvg0","colab_type":"text"},"source":["The training and testing functions"]},{"cell_type":"code","metadata":{"id":"Se1yEaxmey6Z","colab_type":"code","colab":{}},"source":["def train(learning_rate, image_width, image_height, n_train_classes, n_test_classes, restore_training, \\\n","         num_epochs, n_classes, batch_size, seq_length, num_memory_slots, augment, save_dir, model_path, tensorboard_dir):\n","  \n","  # We always use one-hot encoding of the labels in this experiment\n","  label_type = \"one_hot\"\n","\n","  # Initialize the model\n","  model = NTMOneShotLearningModel(model=model_path, n_classes=n_classes,\\\n","                    batch_size=batch_size, seq_length=seq_length,\\\n","                    image_width=image_width, image_height=image_height, \\\n","                    rnn_size=rnn_size, num_memory_slots=num_memory_slots,\\\n","                    rnn_num_layers=rnn_num_layers, read_head_num=read_head_num,\\\n","                    write_head_num=write_head_num, memory_vector_dim=memory_vector_dim,\\\n","                    learning_rate=learning_rate)\n","  print(\"Model initialized\")\n","  data_loader = OmniglotDataLoader(\n","      image_size=(image_width, image_height),\n","      n_train_classses=n_train_classes,\n","      n_test_classes=n_test_classes\n","  )\n","  print(\"Data loaded\")\n","  # Note: our training loop is in the tensorflow 1.x style\n","  with tf.Session() as sess:\n","    if restore_training:\n","      saver = tf.train.Saver()\n","      ckpt = tf.train.get_checkpoint_state(save_dir + '/' + model_path)\n","      saver.restore(sess, ckpt.model_checkpoint_path)\n","    else:\n","      saver = tf.train.Saver(tf.global_variables())\n","      tf.global_variables_initializer().run()\n","    train_writer = tf.summary.FileWriter(tensorboard_dir + '/' + model_path, sess.graph)\n","    print(\"1st\\t2nd\\t3rd\\t4th\\t5th\\t6th\\t7th\\t8th\\t9th\\t10th\\tepoch\\tloss\")\n","    for b in range(num_epochs):\n","      # Test the model\n","      if b % 100 == 0:\n","        # Note: the images are flattened to 1D tensors\n","        # The input data structure is of the following form:\n","        # x_image[i,j,:] = jth image in the ith sequence (or, episode)\n","        # And the sequence of 50 images x_image[i,:,:] constitute\n","        # one episode, and each class (out of 5 classes) has around 10\n","        # appearances in this sequence, as seq_length = 50 and \n","        # n_classes = 5, as specified in the code block below\n","        # See the details in utils.py, OmniglotDataLoader class\n","        x_image, x_label, y = data_loader.fetch_batch(n_classes, batch_size, seq_length,\n","                                  type='test',\n","                                  augment=augment,\n","                                  label_type=label_type)\n","        feed_dict = {model.x_image: x_image, model.x_label: x_label, model.y: y}\n","        output, learning_loss = sess.run([model.o, model.learning_loss], feed_dict=feed_dict)\n","        merged_summary = sess.run(model.learning_loss_summary, feed_dict=feed_dict)\n","        train_writer.add_summary(merged_summary, b)\n","        accuracy = test(seq_length, y, output)\n","        for accu in accuracy:\n","          print('%.4f' % accu, end='\\t')\n","        print('%d\\t%.4f' % (b, learning_loss))\n","\n","      # Save model per 2000 epochs\n","      if b%2000==0 and b>0:\n","        saver.save(sess, save_dir + '/' + model_path + '/model.tfmodel', global_step=b)\n","\n","      # Train the model\n","      x_image, x_label, y = data_loader.fetch_batch(n_classes, batch_size, seq_length, \\\n","                                type='train',\n","                                augment=augment,\n","                                label_type=label_type)\n","      feed_dict = {model.x_image: x_image, model.x_label: x_label, model.y: y}\n","      sess.run(model.train_op, feed_dict=feed_dict)\n","\n","# Fill in this function. You might not need seq_length (the length of an episode)\n","# as an input, depending on your setup \n","# Note: y is the true labels, and of shape (batch_size, seq_length, 5)\n","# output is the network's classification labels\n","def test(seq_length, y, output):\n","  # Fill in\n","\n","  return # Fill in\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VruOInLHkZUK","colab_type":"code","colab":{}},"source":["restore_training = False\n","label_type = \"one_hot\"\n","n_classes = 5\n","seq_length = 50\n","augment = True\n","read_head_num = 4\n","batch_size = 16\n","num_epochs = 100000\n","learning_rate = 1e-3\n","rnn_size = 200\n","image_width = 20\n","image_height = 20\n","rnn_num_layers = 1\n","num_memory_slots = 128\n","memory_vector_dim = 40\n","shift_range = 1\n","write_head_num = 4\n","test_batch_num = 100\n","n_train_classes = 220\n","n_test_classes = 60\n","save_dir = './save/one_shot_learning'\n","tensorboard_dir = './summary/one_shot_learning'\n","model_path = 'MANN'\n","train(learning_rate, image_width, image_height, n_train_classes, n_test_classes, restore_training, \\\n","         num_epochs, n_classes, batch_size, seq_length, num_memory_slots, augment, save_dir, model_path, tensorboard_dir)\n"],"execution_count":0,"outputs":[]}]}