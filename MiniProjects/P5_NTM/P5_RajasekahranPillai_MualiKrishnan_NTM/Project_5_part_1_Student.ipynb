{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Project_5_part_1_Student.ipynb","provenance":[{"file_id":"17BvkhrvTKBi0IMM1Fazl6UVLtm0fb8E7","timestamp":1573460220489},{"file_id":"1tRHaZoIYsfB16Gh-s_KAX6A41NdP3XKa","timestamp":1573332369516},{"file_id":"1EfocEfdlVns48iXbXJf7_Jy-v-WCN9NN","timestamp":1573327062889}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"Ep1Ky-nDBZIr","colab_type":"text"},"source":["You might need to modify the third line in the code cell below, to make sure you cd to the actual directory which your ipynb file is located in.\n","\n","**Caution**: due to the nature of this project's setup, everytime you want to rerun some code cell below, please click **Runtime -> Restart and run all**; this operation clears the computational graphs and the local variables but allow training and testing data that are already loaded from google drive to stay in the colab runtime space. Please do **not** do the following if you just wish to rerun code: click Runtime -> reset all runtimes, and then click Runtime -> Run all; it will remount your google drive, and remove the training and testing data already loaded in your colab runtime space. **Runtime -> Restart and run all** automatically avoids remounting the drive after the first time you run the notebook file; the loaded data can usually stay in your colab runtime space for many hours.\n","\n","Loading the training and testing data after remounting your google drive takes 30 - 40 minutes."]},{"cell_type":"code","metadata":{"id":"7sWni0FseVUz","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount(\"/content/gdrive/\", force_remount=True)\n","%cd gdrive/My Drive/Neural_Turing_Machine/NTM_small"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"R9gdekJg_-xa","colab_type":"code","colab":{}},"source":["from utils import OmniglotDataLoader, one_hot_decode, five_hot_decode\n","import tensorflow as tf\n","import argparse\n","import numpy as np\n","%tensorflow_version 1.x\n","print(tf.__version__)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"otNm4yidAQQB","colab_type":"text"},"source":["Already implemented, no need to change.\n","\n","This class is part of the training loop."]},{"cell_type":"code","metadata":{"id":"MZTXPodW_5_i","colab_type":"code","colab":{}},"source":["class NTMOneShotLearningModel():\n","  def __init__(self, model, n_classes, batch_size, seq_length, image_width, image_height,\n","                rnn_size, num_memory_slots, rnn_num_layers, read_head_num, write_head_num, memory_vector_dim, learning_rate):\n","    self.output_dim = n_classes\n","\n","    # Note: the images are flattened to 1D tensors\n","    # The input data structure is of the following form:\n","    # self.x_image[i,j,:] = jth image in the ith sequence (or, episode)\n","    self.x_image = tf.placeholder(dtype=tf.float32, shape=[batch_size, seq_length, image_width * image_height])\n","    # Model's output label is one-hot encoded\n","    # The data structure is of the following form:\n","    # self.x_label[i,j,:] = one-hot label of the jth image in \n","    #             the ith sequence (or, episode)\n","    self.x_label = tf.placeholder(dtype=tf.float32, shape=[batch_size, seq_length, self.output_dim])\n","    # Target label is one-hot encoded\n","    self.y = tf.placeholder(dtype=tf.float32, shape=[batch_size, seq_length, self.output_dim])\n","    \n","    # The dense layer for mapping controller output and retrieved\n","    # memory content to classification labels\n","    self.controller_output_to_ntm_output = tf.keras.layers.Dense(units=self.output_dim, use_bias=True)\n","\n","    if model == 'LSTM':\n","      # Using a LSTM layer to serve as the controller, no memory\n","      def rnn_cell(rnn_size):\n","        return tf.nn.rnn_cell.BasicLSTMCell(rnn_size)\n","      cell = tf.nn.rnn_cell.MultiRNNCell([rnn_cell(rnn_size) for _ in range(rnn_num_layers)])\n","      state = cell.zero_state(batch_size=batch_size, dtype=tf.float32)\n","    \n","    # Initialize the controller model, including wiping its memory\n","    # Also, get the initial state of the MANN model\n","    \n","    self.state_list = [state]\n","    # Setup the NTM's output\n","    self.o = []\n","    \n","    # Now iterate over every sample in the sequence \n","    for t in range(seq_length):\n","      output, state = cell(tf.concat([self.x_image[:, t, :], self.x_label[:, t, :]], axis=1), state)\n","      # Map controller output (with retrieved memory) + current (offseted) label \n","      # to the overall ntm's output with an affine operation\n","      # The output is the classification labels\n","      output = self.controller_output_to_ntm_output(output)\n","      output = tf.nn.softmax(output, axis=1)\n","      self.o.append(output)\n","      self.state_list.append(state)\n","    # post-process the output of the classifier\n","    self.o = tf.stack(self.o, axis=1)\n","    self.state_list.append(state)\n","\n","    eps = 1e-8\n","    # cross entropy, between model output labels and target labels\n","    self.learning_loss = -tf.reduce_mean(  \n","        tf.reduce_sum(self.y * tf.log(self.o + eps), axis=[1, 2])\n","    )\n","    \n","    self.o = tf.reshape(self.o, shape=[batch_size, seq_length, -1])\n","    self.learning_loss_summary = tf.summary.scalar('learning_loss', self.learning_loss)\n","\n","    with tf.variable_scope('optimizer'):\n","      self.optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n","      self.train_op = self.optimizer.minimize(self.learning_loss)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"X_qMlbTWAvg0","colab_type":"text"},"source":["The training and testing functions"]},{"cell_type":"code","metadata":{"id":"Se1yEaxmey6Z","colab_type":"code","colab":{}},"source":["def train(learning_rate, image_width, image_height, n_train_classes, n_test_classes, restore_training, \\\n","         num_epochs, n_classes, batch_size, seq_length, num_memory_slots, augment, save_dir, model_path, tensorboard_dir):\n","  \n","  # We always use one-hot encoding of the labels in this experiment\n","  label_type = \"one_hot\"\n","\n","  # Initialize the model\n","  model = NTMOneShotLearningModel(model=model_path, n_classes=n_classes,\\\n","                    batch_size=batch_size, seq_length=seq_length,\\\n","                    image_width=image_width, image_height=image_height, \\\n","                    rnn_size=rnn_size, num_memory_slots=num_memory_slots,\\\n","                    rnn_num_layers=rnn_num_layers, read_head_num=read_head_num,\\\n","                    write_head_num=write_head_num, memory_vector_dim=memory_vector_dim,\\\n","                    learning_rate=learning_rate)\n","  print(\"Model initialized\")\n","  data_loader = OmniglotDataLoader(\n","      image_size=(image_width, image_height),\n","      n_train_classses=n_train_classes,\n","      n_test_classes=n_test_classes\n","  )\n","  print(\"Data loaded\")\n","  # Note: our training loop is in the tensorflow 1.x style\n","  with tf.Session() as sess:\n","    if restore_training:\n","      saver = tf.train.Saver()\n","      ckpt = tf.train.get_checkpoint_state(save_dir + '/' + model_path)\n","      saver.restore(sess, ckpt.model_checkpoint_path)\n","    else:\n","      saver = tf.train.Saver(tf.global_variables())\n","      tf.global_variables_initializer().run()\n","    train_writer = tf.summary.FileWriter(tensorboard_dir + '/' + model_path, sess.graph)\n","    print(\"1st\\t2nd\\t3rd\\t4th\\t5th\\t6th\\t7th\\t8th\\t9th\\t10th\\tepoch\\tloss\")\n","    for b in range(num_epochs):\n","      # Test the model\n","      if b % 100 == 0:\n","        # Note: the images are flattened to 1D tensors\n","        # The input data structure is of the following form:\n","        # x_image[i,j,:] = jth image in the ith sequence (or, episode)\n","        # And the sequence of 50 images x_image[i,:,:] constitute\n","        # one episode, and each class (out of 5 classes) has around 10\n","        # appearances in this sequence, as seq_length = 50 and \n","        # n_classes = 5, as specified in the code block below\n","        # See the details in utils.py, OmniglotDataLoader class\n","        x_image, x_label, y = data_loader.fetch_batch(n_classes, batch_size, seq_length,\n","                                  type='test',\n","                                  augment=augment,\n","                                  label_type=label_type)\n","        feed_dict = {model.x_image: x_image, model.x_label: x_label, model.y: y}\n","        output, learning_loss = sess.run([model.o, model.learning_loss], feed_dict=feed_dict)\n","        merged_summary = sess.run(model.learning_loss_summary, feed_dict=feed_dict)\n","        train_writer.add_summary(merged_summary, b)\n","        accuracy = test(seq_length, y, output)\n","        for accu in accuracy:\n","          print('%.4f' % accu, end='\\t')\n","        print('%d\\t%.4f' % (b, learning_loss))\n","\n","      # Save model per 2000 epochs\n","      if b%2000==0 and b>0:\n","        saver.save(sess, save_dir + '/' + model_path + '/model.tfmodel', global_step=b)\n","\n","      # Train the model\n","      x_image, x_label, y = data_loader.fetch_batch(n_classes, batch_size, seq_length, \\\n","                                type='train',\n","                                augment=augment,\n","                                label_type=label_type)\n","      feed_dict = {model.x_image: x_image, model.x_label: x_label, model.y: y}\n","      sess.run(model.train_op, feed_dict=feed_dict)\n","      \n","# Fill in this function. You might not need seq_length (the length of an episode)\n","# as an input, depending on your setup \n","# Note: y is the true labels, and of shape (batch_size, seq_length, 5)\n","# output is the network's classification labels\n","def test(seq_length, y, output):\n","  # Fill in\n","\n","  return # Fill in"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VruOInLHkZUK","colab_type":"code","colab":{}},"source":["restore_training = False\n","label_type = \"one_hot\"\n","n_classes = 5\n","seq_length = 50\n","augment = True\n","read_head_num = 4\n","batch_size = 16\n","num_epochs = 100000\n","learning_rate = 1e-3\n","rnn_size = 200\n","image_width = 20\n","image_height = 20\n","rnn_num_layers = 1\n","num_memory_slots = 128\n","memory_vector_dim = 40\n","shift_range = 1\n","write_head_num = 4\n","test_batch_num = 100\n","n_train_classes = 220\n","n_test_classes = 60\n","save_dir = './save/one_shot_learning'\n","tensorboard_dir = './summary/one_shot_learning'\n","model_path = 'LSTM'\n","train(learning_rate, image_width, image_height, n_train_classes, n_test_classes, restore_training, \\\n","         num_epochs, n_classes, batch_size, seq_length, num_memory_slots, augment, save_dir, model_path, tensorboard_dir)\n"],"execution_count":0,"outputs":[]}]}